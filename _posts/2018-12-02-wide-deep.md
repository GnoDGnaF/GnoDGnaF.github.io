---
layout: post
title: "Wide & Deep Learning for Recommender Systems (2016)"
categories: papers
modify: 2018-12-02 21:15:04
published: true
---

> In this paper, we present Wide & Deep learning — **jointly trained** wide linear models and deep neural networks — to combine the benefits of memorization and generalization for recommender systems.


[hypothesis](chrome-extension://bjfhmglciegochdpefhhlphglcehbmek/content/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.07792.pdf)

### 1. 推荐系统简介

一个完整的推荐系统主要分为两个部分：**retrieval**和**ranking**，如图1所示（The retrieval system returns <font color="blue">a short list of items that best match the query using various signals</font> , usually a combination of machine-learned models and human-defined rules; <font color="red">the ranking system ranks all items by their scores</font>）。

<center>
<img src="../img/wide-deep/00-overview.png" style="width:70%;height:70%;">  
<br>
图1 Overview of the recommender system 
</center>
<br>

### 2. WIDE & DEEP LEARNING

该论文提出了一种用于**ranking**模块的`Wide & Deep learning framework`，如图2所示。

<center>
<img src="../img/wide-deep/02-wide-deep.png" style="width:100%;height:100%;">  
<br>
图2 The spectrum of Wide & Deep models
</center>
<br>

该框架包括两个部分：`Wide Component`和`Deep Component`。那么，为什么需要这两个部分呢？论文中给出了解释：

- Memorization of feature interactions through a ***wide*** set of cross-product feature transformations are <font color="blue">effective and interpretable</font>, while generalization requires more feature engineering effort
- With less feature engineering, ***deep*** neural networks can <font color="blue">generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features</font>

另外，在整个框架中，`Wide Component`和`Deep Component`进行**joint training**，即两者的结果输入到一个logistic loss function中，如图3所示，因此在训练过程中同时更新各自的参数。

<center>
<img src="../img/wide-deep/04-equation.png" style="width:60%;height:60%;">  
<br>
图3 Wide & Deep model
</center>
<br>

该论文以Apps推荐为例，给出了Wide & Deep model的落地方案。图4为该Apps推荐系统的pipeline，主要包括`Data Generation`、`Model Training`以及`Model Serving`这三个部分：

- `Data Generation`：用于生成训练数据
- `Model Training`：模型训练，模型的具体结构如图5所示
- `Model Serving`：模型部署（响应时间为10 ms左右）

<center>
<img src="../img/wide-deep/01-pipeline.png" style="width:60%;height:60%;">  
<br>
图4 Apps recommendation pipeline overview
</center>
<br>

<center>
<img src="../img/wide-deep/03-model.png" style="width:60%;height:60%;">  
<br>
图5 Wide & Deep model structure for apps recommendation
</center>
<br>

图6展示了`Offline AUC/Online Acquisition Gain`的实验结果。

<center>
<img src="../img/wide-deep/05-result.png" style="width:60%;height:60%;">  
<br>
图6 Offline & online metrics of different models
</center>
<br>

有意思的是`Deep`的`Offline AUC`比`Wide`要低，但是其`Online Acquisition Gain`比`Wide`要高2.9%。对于这一现象可能有几种解释：
- 相比`Deep`，`Wide`更易在Offline的数据集上过度学习，即`overfit`
- Offline metrics与Online metrics不线性相关

总之，如何设计Offline metrics或者offline测试也是一个重要的研究课题。

图7展示了`Serving Latency`的实验结果，显然，`Serving Latency`主要依赖于Batch size和Number of Threads。

<center>
<img src="../img/wide-deep/06-serve.png" style="width:60%;height:60%;">  
<br>
图7 Serving latency
</center>
<br>

### 3. 总结

- `Wide & Deep model structure`：在`Wide`的基础上，引入`Deep`模块用于特征提取（Wide linear models can effectively **memorize sparse feature interactions** using cross-product feature transformations; deep neural networks can **generalize to previously unseen feature interactions** through low-dimensional embeddings）
- `joint training`：与ensemble和stacking等模型训练方式相比，`joint training`是一种新颖的模型训练方式
